{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPLH5+wnYxTMWjRgugQI/hL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/salankyekru-lang/Master-Thesis---Supplementary-Files/blob/main/Data%20cleanup%20and%20analysis%20script\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9u8PmYExEU6G"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "BASE = \"/content/drive/MyDrive\"\n",
        "OUT  = os.path.join(BASE, \"sep15.3.0\")\n",
        "os.makedirs(OUT, exist_ok=True)\n",
        "\n",
        "SEARCH_ROOTS = [BASE, f\"{BASE}/2ndtrial\", f\"{BASE}/My Drive - 2ndtrial\", f\"{BASE}/outputs_portfolio\"]\n",
        "\n",
        "# ---------- Helpers: file finding, dates, columns ----------\n",
        "def find_file(preferred_rel_paths, alt_patterns=None, required=True, label=\"file\"):\n",
        "    for rel in preferred_rel_paths:\n",
        "        for root in SEARCH_ROOTS:\n",
        "            p = os.path.join(root, rel)\n",
        "            if os.path.exists(p):\n",
        "                print(f\"âœ“ Found {label}: {p}\")\n",
        "                return p\n",
        "    if alt_patterns:\n",
        "        for root in SEARCH_ROOTS:\n",
        "            for pat in alt_patterns:\n",
        "                hits = glob.glob(os.path.join(root, pat))\n",
        "                if hits:\n",
        "                    p = max(hits, key=os.path.getmtime)\n",
        "                    print(f\"âœ“ Found {label} (pattern): {p}\")\n",
        "                    return p\n",
        "    if required:\n",
        "        raise FileNotFoundError(f\"Missing {label}. Looked for {preferred_rel_paths} and {alt_patterns}\")\n",
        "    else:\n",
        "        print(f\"â€¦ {label} not found (optional).\")\n",
        "        return None\n",
        "\n",
        "def parse_date_series(s, prefer_dayfirst=False):\n",
        "    \"\"\"Robustly parse messy date series into datetime.date\"\"\"\n",
        "    dt = pd.to_datetime(s, errors='coerce', dayfirst=prefer_dayfirst, infer_datetime_format=True)\n",
        "    bad = dt.isna()\n",
        "    if bad.any():\n",
        "        dt2 = pd.to_datetime(pd.Series(s)[bad], errors='coerce',\n",
        "                             dayfirst=not prefer_dayfirst, infer_datetime_format=True)\n",
        "        dt.loc[bad] = dt2.values\n",
        "    return pd.to_datetime(dt.dt.date)\n",
        "\n",
        "def ensure_datetime_index(df, date_col, prefer_dayfirst=False):\n",
        "    if date_col not in df.columns:\n",
        "        raise ValueError(f\"Expected date column '{date_col}' not found in {df.columns.tolist()}\")\n",
        "    df = df.copy()\n",
        "    df[\"Date\"] = parse_date_series(df[date_col], prefer_dayfirst)\n",
        "    df = df.dropna(subset=[\"Date\"]).set_index(\"Date\").sort_index()\n",
        "    return df\n",
        "\n",
        "def normalize_columns(df):\n",
        "    \"\"\"Trim, collapse spaces, remove NBSP, and apply explicit fixes for known headers.\"\"\"\n",
        "    def _norm(cols):\n",
        "        return (pd.Index(cols).astype(str)\n",
        "                .str.replace('\\u00A0', ' ', regex=False)  # NBSP -> space\n",
        "                .str.replace(r'\\s+', ' ', regex=True)     # collapse multi-spaces\n",
        "                .str.strip())\n",
        "    raw_cols = list(df.columns)\n",
        "    df.columns = _norm(df.columns)\n",
        "\n",
        "    # explicit fixes for trailing-space headers you showed\n",
        "    explicit_fixes = {\n",
        "        \"Nel ASA \": \"Nel ASA\",\n",
        "        \"China Longyuan Power Group \": \"China Longyuan Power Group\",\n",
        "        \"Adani Green Energy \": \"Adani Green Energy\",\n",
        "    }\n",
        "    # apply fixes both before & after normalization (defensive)\n",
        "    df.rename(columns={k:v for k,v in explicit_fixes.items() if k in raw_cols}, inplace=True)\n",
        "    df.rename(columns={k:v for k,v in explicit_fixes.items() if k in df.columns}, inplace=True)\n",
        "    df.columns = _norm(df.columns)\n",
        "    return df\n",
        "\n",
        "def ann_mean(x): return x.mean() * 252.0\n",
        "def ann_vol(x):  return x.std()  * np.sqrt(252.0)\n",
        "\n",
        "def max_drawdown_stats(returns):\n",
        "    w = (1 + returns.fillna(0)).cumprod()\n",
        "    runmax = w.cummax()\n",
        "    dd = w / runmax - 1.0\n",
        "    trough = dd.idxmin() if len(dd)>0 else pd.NaT\n",
        "    mdd = float(dd.loc[trough]) if len(dd)>0 else np.nan\n",
        "    peak = w.loc[:trough].idxmax() if pd.notna(trough) else pd.NaT\n",
        "    recov = dd.loc[trough:] if pd.notna(trough) else dd\n",
        "    recov_date = recov.index[recov.values >= 0][:1]\n",
        "    recov_date = recov_date[0] if len(recov_date) else pd.NaT\n",
        "    return mdd, peak, trough, recov_date, w, dd\n",
        "\n",
        "print(\"==> Output folder:\", OUT)\n",
        "\n",
        "# ------------------ Portfolio mapping (UPDATED) ------------------\n",
        "bucket_cols = [\"developed_mature\",\"developed_emerging\",\"developing_mature\",\"developing_emerging\"]\n",
        "bucket_map = {\n",
        "    \"Vestas Wind\": \"developed_mature\",\n",
        "    \"Oersted AS\": \"developed_mature\",\n",
        "    \"NextEra Energy\": \"developed_mature\",    # moved here\n",
        "    \"Nel ASA\": \"developed_emerging\",         # moved here\n",
        "    \"Bloom Energy\": \"developed_emerging\",\n",
        "    \"Plug Power\": \"developed_emerging\",\n",
        "    \"AC Energy Philippines Inc\": \"developing_mature\",\n",
        "    \"Tata Power\": \"developing_mature\",\n",
        "    \"China Longyuan Power Group\": \"developing_mature\",\n",
        "    \"ACWA Power\": \"developing_emerging\",\n",
        "    \"Adani Green Energy\": \"developing_emerging\",\n",
        "    \"Sembcorp\": \"developing_emerging\",\n",
        "}\n",
        "\n",
        "# FX columns in Workpaper_2 (1 USD = X local) -> USD price = local / (USD/local)\n",
        "fx_map = {\n",
        "    \"Vestas Wind\": \"USD/DKK\",\n",
        "    \"Oersted AS\": \"USD/DKK\",\n",
        "    \"Nel ASA\": \"USD/NOK\",\n",
        "    \"Bloom Energy\": None,         # already USD\n",
        "    \"NextEra Energy\": None,       # already USD\n",
        "    \"Plug Power\": None,           # already USD\n",
        "    \"AC Energy Philippines Inc\": \"USD/PHP\",\n",
        "    \"Tata Power\": \"USD/INR\",\n",
        "    \"China Longyuan Power Group\": \"USD/HKD\",\n",
        "    \"ACWA Power\": \"USD/SAR\",\n",
        "    \"Adani Green Energy\": \"USD/INR\",\n",
        "    \"Sembcorp\": \"USD/SGD\",\n",
        "}\n",
        "\n",
        "# ------------------ Load Workpaper_2 & build USD prices/returns ------------------\n",
        "wp_path = find_file(\n",
        "    [\"Workpaper_2.xlsx\", \"workpaper_2.xlsx\", \"2ndtrial/Workpaper_2.xlsx\", \"2ndtrial/workpaper_2.xlsx\", \"Workpaper.xlsx\"],\n",
        "    alt_patterns=[\"**/Workpaper_2.xlsx\",\"**/workpaper_2.xlsx\",\"**/Workpaper.xlsx\"],\n",
        "    label=\"Workpaper_2 (prices + FX)\"\n",
        ")\n",
        "wp = pd.read_excel(wp_path)\n",
        "wp = normalize_columns(wp)                  # <â€” normalize headers FIRST\n",
        "wp = ensure_datetime_index(wp, \"Date\", prefer_dayfirst=False)\n",
        "\n",
        "print(\"Columns after cleaning:\", wp.columns.tolist())\n",
        "\n",
        "stocks = list(bucket_map.keys())\n",
        "missing_stocks = [s for s in stocks if s not in wp.columns]\n",
        "if missing_stocks:\n",
        "    import difflib\n",
        "    suggestions = {s: difflib.get_close_matches(s, list(wp.columns), n=3) for s in missing_stocks}\n",
        "    raise ValueError(f\"Workpaper missing stock columns: {missing_stocks}\\nClosest matches: {suggestions}\")\n",
        "\n",
        "needed_fx = sorted(set([c for c in fx_map.values() if c is not None]))\n",
        "missing_fx = [f for f in needed_fx if f not in wp.columns]\n",
        "if missing_fx:\n",
        "    raise ValueError(f\"Workpaper missing FX columns: {missing_fx}\")\n",
        "\n",
        "# forward-fill prices & FX (no backfill pre-IPO)\n",
        "wp_ff = wp.copy()\n",
        "wp_ff[stocks + needed_fx] = wp_ff[stocks + needed_fx].apply(pd.to_numeric, errors=\"coerce\").ffill()\n",
        "\n",
        "# USD price = local / (USD/local) ; USD-native = local\n",
        "usd_prices = pd.DataFrame(index=wp_ff.index)\n",
        "for s in stocks:\n",
        "    fx = fx_map[s]\n",
        "    if fx is None:\n",
        "        usd_prices[s] = wp_ff[s].astype(float)\n",
        "    else:\n",
        "        usd_prices[s] = wp_ff[s].astype(float) / wp_ff[fx].astype(float)\n",
        "\n",
        "usd_returns = usd_prices.pct_change()\n",
        "\n",
        "# Save audit files\n",
        "usd_prices.reset_index().rename(columns={\"index\":\"Date\"}).to_excel(os.path.join(OUT, \"workpaper2_usd_prices_by_stock.xlsx\"), index=False)\n",
        "usd_returns.reset_index().rename(columns={\"index\":\"Date\"}).to_excel(os.path.join(OUT, \"workpaper2_usd_returns_by_stock.xlsx\"), index=False)\n",
        "print(\"âœ… Saved per-stock USD prices/returns for audit.\")\n",
        "\n",
        "# ------------------ Build bucket equal-weight returns ------------------\n",
        "buckets = pd.DataFrame(index=usd_returns.index, columns=bucket_cols, dtype=float)\n",
        "for b in bucket_cols:\n",
        "    members = [s for s, bb in bucket_map.items() if bb == b]\n",
        "    buckets[b] = usd_returns[members].mean(axis=1, skipna=True)\n",
        "\n",
        "bucket_file = os.path.join(OUT, \"bucket_log_returns.xlsx\")\n",
        "buckets.reset_index().rename(columns={\"index\":\"Date\"}).to_excel(bucket_file, index=False)\n",
        "print(f\"âœ… Saved bucket returns: {bucket_file}\")\n",
        "\n",
        "# ------------------ Load factors, proxies, policy ------------------\n",
        "# FF5 (US) + RF\n",
        "ff5_path = find_file([\"FF5.csv\"], label=\"FF5.csv\")\n",
        "ff5_raw = pd.read_csv(ff5_path)\n",
        "ff5_raw = normalize_columns(ff5_raw)\n",
        "ff5 = ensure_datetime_index(ff5_raw, \"Date\", prefer_dayfirst=False)\n",
        "need_ff = [\"Mkt-RF\",\"SMB\",\"HML\",\"RMW\",\"CMA\",\"RF\"]\n",
        "for c in need_ff:\n",
        "    if c not in ff5.columns: raise ValueError(f\"FF5 missing {c}\")\n",
        "ff5 = ff5[need_ff].astype(float)\n",
        "if ff5[\"RF\"].dropna().mean() > 0.01:\n",
        "    print(\"âš ï¸ RF appears in percent; converting by /100.\")\n",
        "    ff5[\"RF\"] = ff5[\"RF\"]/100.0\n",
        "\n",
        "# Brent\n",
        "brent_path = find_file([\"brent_daily.xlsx\"], label=\"brent_daily.xlsx\")\n",
        "br_raw = pd.read_excel(brent_path)\n",
        "br_raw = normalize_columns(br_raw)\n",
        "br = ensure_datetime_index(br_raw, \"observation_date\" if \"observation_date\" in br_raw.columns else \"Date\", prefer_dayfirst=False)\n",
        "br_col = \"Brent_ret\" if \"Brent_ret\" in br.columns else (\"Brent_re\" if \"Brent_re\" in br.columns else (\"Log_returns\" if \"Log_returns\" in br.columns else None))\n",
        "if br_col is None: raise ValueError(\"brent_daily.xlsx must contain Brent_ret/Brent_re/Log_returns\")\n",
        "br[\"Brent_ret\"] = pd.to_numeric(br[br_col], errors=\"coerce\")\n",
        "br = br[[\"Brent_ret\"]]\n",
        "\n",
        "# DGS10\n",
        "dgs_path = find_file([\"DGS10.xlsx\",\"dgs10_daily.xlsx\"], alt_patterns=[\"**/DGS10*.xlsx\",\"**/dgs10*.xlsx\"], label=\"DGS10\")\n",
        "dgs_raw = pd.read_excel(dgs_path)\n",
        "dgs_raw = normalize_columns(dgs_raw)\n",
        "dgs = ensure_datetime_index(dgs_raw, \"observation_date\" if \"observation_date\" in dgs_raw.columns else \"Date\", prefer_dayfirst=False)\n",
        "if \"Delta_bp\" in dgs.columns:\n",
        "    dgs[\"DGS10_deltabp\"] = pd.to_numeric(dgs[\"Delta_bp\"], errors=\"coerce\")\n",
        "elif \"delta_bp\" in dgs.columns:\n",
        "    dgs[\"DGS10_deltabp\"] = pd.to_numeric(dgs[\"delta_bp\"], errors=\"coerce\")\n",
        "else:\n",
        "    raise ValueError(\"DGS10 file must include Delta_bp or delta_bp\")\n",
        "dgs = dgs[[\"DGS10_deltabp\"]]\n",
        "\n",
        "# ACWI (optional)\n",
        "acwi_path = find_file([\"ACWI.xlsx\",\"ACWI_DAILY.xlsx\",\"ICWI.xlsx\"], alt_patterns=[\"**/ACWI*.xlsx\",\"**/ICWI*.xlsx\"], required=False, label=\"ACWI\")\n",
        "acwi = None\n",
        "if acwi_path:\n",
        "    ac_raw = pd.read_excel(acwi_path)\n",
        "    ac_raw = normalize_columns(ac_raw)\n",
        "    ac = ensure_datetime_index(ac_raw, \"Date\", prefer_dayfirst=False)\n",
        "    if \"ACWI_ret\" in ac.columns:\n",
        "        ac[\"ACWI_ret\"] = pd.to_numeric(ac[\"ACWI_ret\"], errors=\"coerce\")\n",
        "        acwi = ac[[\"ACWI_ret\"]]\n",
        "    elif \"Price\" in ac.columns:\n",
        "        ac[\"ACWI_ret\"] = ac[\"Price\"].astype(float).pct_change()\n",
        "        acwi = ac[[\"ACWI_ret\"]]\n",
        "\n",
        "# Policy (use GlobalPolicy_0 if present; else 3day window)\n",
        "pol_path = find_file(\n",
        "    [\"policy_global_expanded.xlsx\",\"policy_global_expanded_pm3.xlsx\",\"outputs_portfolio/policy_global_expanded.xlsx\"],\n",
        "    alt_patterns=[\"**/policy_global_expanded*.xlsx\"],\n",
        "    label=\"policy_global_expanded\"\n",
        ")\n",
        "pol_raw = pd.read_excel(pol_path)\n",
        "pol_raw = normalize_columns(pol_raw)\n",
        "pol = ensure_datetime_index(pol_raw, \"Date\", prefer_dayfirst=False)\n",
        "use_col = \"GlobalPolicy_0\" if \"GlobalPolicy_0\" in pol.columns else (\"GlobalPolicy_3day_window\" if \"GlobalPolicy_3day_window\" in pol.columns else (\"GlobalPolicy_win_pm3\" if \"GlobalPolicy_win_pm3\" in pol.columns else None))\n",
        "if use_col is None:\n",
        "    raise ValueError(\"Policy file must include GlobalPolicy_0 or GlobalPolicy_3day_window/GlobalPolicy_win_pm3\")\n",
        "pol = pol[[use_col]].rename(columns={use_col:\"GlobalPolicy_0\"})\n",
        "pol[\"GlobalPolicy_0\"] = pol[\"GlobalPolicy_0\"].fillna(0).astype(int)\n",
        "\n",
        "# ------------------ Part 1: Risk/Return ------------------\n",
        "summary_rows, wealth_map, dd_map = [], {}, {}\n",
        "for b in bucket_cols:\n",
        "    r = buckets[b].astype(float)\n",
        "    mu_ann, vol_ann = ann_mean(r), ann_vol(r)\n",
        "    mdd, pk, tr, rc, w, dds = max_drawdown_stats(r)\n",
        "    wealth_map[b], dd_map[b] = w, dds\n",
        "    summary_rows.append({\n",
        "        \"bucket\": b,\n",
        "        \"ann_return\": mu_ann,\n",
        "        \"ann_vol\": vol_ann,\n",
        "        \"max_drawdown\": mdd,\n",
        "        \"dd_peak_date\": pk,\n",
        "        \"dd_trough_date\": tr,\n",
        "        \"dd_recovery_date\": rc\n",
        "    })\n",
        "summary_static = pd.DataFrame(summary_rows).set_index(\"bucket\").round(10)\n",
        "\n",
        "# Rolling (252d)\n",
        "win = 252\n",
        "roll_dict = {}\n",
        "for b in bucket_cols:\n",
        "    r = buckets[b].astype(float)\n",
        "    roll = pd.DataFrame({\n",
        "        f\"{b}_roll_ann_return\": r.rolling(win).mean() * 252.0,\n",
        "        f\"{b}_roll_ann_vol\":    r.rolling(win).std()  * np.sqrt(252.0),\n",
        "        f\"{b}_roll_drawdown\":   (1 + r.fillna(0)).cumprod() / (1 + r.fillna(0)).cumprod().cummax() - 1.0\n",
        "    })\n",
        "    roll_dict[b] = roll\n",
        "rolling_metrics = pd.concat(roll_dict.values(), axis=1)\n",
        "\n",
        "with pd.ExcelWriter(os.path.join(OUT,\"rolling_metrics_252d.xlsx\"), engine=\"openpyxl\") as xw:\n",
        "    summary_static.to_excel(xw, sheet_name=\"summary_static\")\n",
        "    rolling_metrics.to_excel(xw, sheet_name=\"rolling_252d\")\n",
        "\n",
        "# Plots: wealth & drawdowns\n",
        "for b in bucket_cols:\n",
        "    plt.figure(figsize=(10,5))\n",
        "    plt.plot(wealth_map[b].index, wealth_map[b].values, label=b)\n",
        "    plt.title(f\"Wealth â€” {b}\"); plt.xlabel(\"Date\"); plt.tight_layout()\n",
        "    plt.savefig(os.path.join(OUT, f\"wealth_{b}.png\"), dpi=150); plt.close()\n",
        "\n",
        "    plt.figure(figsize=(10,4))\n",
        "    plt.plot(dd_map[b].index, dd_map[b].values)\n",
        "    plt.title(f\"Drawdown â€” {b}\"); plt.xlabel(\"Date\"); plt.tight_layout()\n",
        "    plt.savefig(os.path.join(OUT, f\"drawdown_{b}.png\"), dpi=150); plt.close()\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "for b in bucket_cols:\n",
        "    plt.plot(wealth_map[b].index, wealth_map[b].values, label=b)\n",
        "plt.title(\"Wealth â€” All Buckets\"); plt.xlabel(\"Date\"); plt.legend(); plt.tight_layout()\n",
        "plt.savefig(os.path.join(OUT, \"wealth_all.png\"), dpi=150); plt.close()\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "for b in bucket_cols:\n",
        "    plt.plot(dd_map[b].index, dd_map[b].values, label=b)\n",
        "plt.title(\"Drawdowns â€” All Buckets\"); plt.xlabel(\"Date\"); plt.legend(); plt.tight_layout()\n",
        "plt.savefig(os.path.join(OUT, \"drawdowns_all.png\"), dpi=150); plt.close()\n",
        "\n",
        "print(\"âœ… Part 1 saved:\", os.path.join(OUT,\"rolling_metrics_252d.xlsx\"))\n",
        "\n",
        "# ------------------ Part 2: Event Study (CAR = sum of ARs) ------------------\n",
        "# Build master on bucket calendar\n",
        "master = buckets.copy()\n",
        "master = master.join(ff5, how=\"left\")\n",
        "master = master.join(br, how=\"left\")\n",
        "master = master.join(dgs, how=\"left\")\n",
        "if acwi is not None:\n",
        "    master = master.join(acwi, how=\"left\")\n",
        "master = master.join(pol, how=\"left\")\n",
        "\n",
        "# RF sanity (decimals)\n",
        "if master[\"RF\"].dropna().mean() > 0.01:\n",
        "    master[\"RF\"] = master[\"RF\"] / 100.0\n",
        "\n",
        "regressors = [\"Mkt-RF\",\"SMB\",\"HML\",\"RMW\",\"CMA\",\"Brent_ret\",\"DGS10_deltabp\"] + ([\"ACWI_ret\"] if \"ACWI_ret\" in master.columns else [])\n",
        "req_cols = bucket_cols + [\"RF\",\"GlobalPolicy_0\",\"Brent_ret\",\"DGS10_deltabp\",\"Mkt-RF\",\"SMB\",\"HML\",\"RMW\",\"CMA\"]\n",
        "if \"ACWI_ret\" in master.columns: req_cols += [\"ACWI_ret\"]\n",
        "master_clean = master.dropna(subset=req_cols)\n",
        "\n",
        "dates  = master_clean.index\n",
        "events = dates[master_clean[\"GlobalPolicy_0\"] != 0]\n",
        "print(\"Events available:\", len(events))\n",
        "\n",
        "WINDOWS = {\"[-1,+1]\":(-1,1), \"[-3,+3]\":(-3,3), \"[0,+5]\":(0,5)}\n",
        "EST_LO, EST_HI = -250, -31\n",
        "PROFILE_BAND = 10\n",
        "\n",
        "def rel_window(t0, lo, hi):\n",
        "    if t0 not in dates: return None\n",
        "    i = dates.get_loc(t0)\n",
        "    lo_i, hi_i = max(i+lo, 0), min(i+hi, len(dates)-1)\n",
        "    return dates[lo_i:hi_i+1]\n",
        "\n",
        "def est_window(t0, lo=EST_LO, hi=EST_HI):\n",
        "    if t0 not in dates: return None\n",
        "    i = dates.get_loc(t0)\n",
        "    lo_i, hi_i = max(i+lo, 0), min(i+hi, len(dates)-1)\n",
        "    if lo_i > hi_i: return None\n",
        "    return dates[lo_i:hi_i+1]\n",
        "\n",
        "# Pure AR profile & CAR = sum(AR)\n",
        "idx = range(-PROFILE_BAND, PROFILE_BAND+1)\n",
        "pure_profiles = {b: pd.Series(0.0, index=idx) for b in bucket_cols}\n",
        "pure_counts   = {b: pd.Series(0,   index=idx, dtype=int) for b in bucket_cols}\n",
        "pure_cars = []\n",
        "\n",
        "for t0 in events:\n",
        "    i0 = dates.get_loc(t0)\n",
        "    for b in bucket_cols:\n",
        "        # AR profile\n",
        "        for k in idx:\n",
        "            j = i0 + k\n",
        "            if 0 <= j < len(dates):\n",
        "                val = float(master_clean.at[dates[j], b])\n",
        "                pure_profiles[b].loc[k] += val\n",
        "                pure_counts[b].loc[k]   += 1\n",
        "        # CAR windows (sum)\n",
        "        for wname,(lo,hi) in WINDOWS.items():\n",
        "            win = rel_window(t0, lo, hi)\n",
        "            if win is None or len(win)==0: continue\n",
        "            car = float(master_clean.loc[win, b].sum())\n",
        "            pure_cars.append({\"event_date\":t0,\"bucket\":b,\"window\":wname,\"CAR\":car})\n",
        "\n",
        "pure_AR = pd.concat([(pure_profiles[b]/pure_counts[b].replace(0,np.nan)).rename(b) for b in bucket_cols], axis=1)\n",
        "pure_CAR_by_event = pd.DataFrame(pure_cars)\n",
        "pure_CAR_mean = pure_CAR_by_event.groupby([\"bucket\",\"window\"])[\"CAR\"].mean().unstack(\"window\") if not pure_CAR_by_event.empty else pd.DataFrame(columns=WINDOWS.keys())\n",
        "\n",
        "# Model-adjusted AR & CAR = sum(AR)\n",
        "model_profiles = {b: pd.Series(0.0, index=idx) for b in bucket_cols}\n",
        "model_counts   = {b: pd.Series(0,   index=idx, dtype=int) for b in bucket_cols}\n",
        "model_cars = []\n",
        "\n",
        "for t0 in events:\n",
        "    est = est_window(t0)\n",
        "    if est is None or len(est) < 60:\n",
        "        continue\n",
        "    X_est = master_clean.loc[est, regressors].astype(float)\n",
        "    Xe = sm.add_constant(X_est, has_constant=\"add\")\n",
        "    for b in bucket_cols:\n",
        "        y_est = (master_clean.loc[est, b] - master_clean.loc[est, \"RF\"]).astype(float)\n",
        "        valid = Xe.notna().all(axis=1) & y_est.notna()\n",
        "        if valid.sum() < 50: continue\n",
        "        mdl = sm.OLS(y_est[valid], Xe[valid]).fit()\n",
        "\n",
        "        # Profile\n",
        "        i0 = dates.get_loc(t0)\n",
        "        for k in idx:\n",
        "            j = i0 + k\n",
        "            if 0 <= j < len(dates):\n",
        "                dt = dates[j]\n",
        "                X_row = sm.add_constant(master_clean.loc[[dt], regressors].astype(float), has_constant=\"add\")\n",
        "                if X_row.notna().all(axis=1).iloc[0]:\n",
        "                    y_act = float(master_clean.at[dt, b] - master_clean.at[dt, \"RF\"])\n",
        "                    y_hat = float(mdl.predict(X_row).iloc[0])\n",
        "                    ar = y_act - y_hat\n",
        "                    model_profiles[b].loc[k] += ar\n",
        "                    model_counts[b].loc[k]   += 1\n",
        "\n",
        "        # CAR windows (sum)\n",
        "        for wname,(lo,hi) in WINDOWS.items():\n",
        "            win = rel_window(t0, lo, hi)\n",
        "            if win is None or len(win)==0: continue\n",
        "            Xw = sm.add_constant(master_clean.loc[win, regressors].astype(float), has_constant=\"add\")\n",
        "            if Xw.notna().all(axis=1).sum() != len(win): continue\n",
        "            y_act = (master_clean.loc[win, b] - master_clean.loc[win, \"RF\"]).astype(float)\n",
        "            y_hat = mdl.predict(Xw)\n",
        "            ar_series = (y_act - y_hat).astype(float)\n",
        "            car = float(ar_series.sum())\n",
        "            model_cars.append({\"event_date\":t0,\"bucket\":b,\"window\":wname,\"CAR\":car})\n",
        "\n",
        "model_AR = pd.concat([(model_profiles[b]/model_counts[b].replace(0,np.nan)).rename(b) for b in bucket_cols], axis=1)\n",
        "model_CAR_by_event = pd.DataFrame(model_cars)\n",
        "model_CAR_mean = model_CAR_by_event.groupby([\"bucket\",\"window\"])[\"CAR\"].mean().unstack(\"window\") if not model_CAR_by_event.empty else pd.DataFrame(columns=WINDOWS.keys())\n",
        "\n",
        "# +1 vs âˆ’1 splits & Welch diff\n",
        "policy_used = pol.rename(columns={\"GlobalPolicy_0\":\"sign\"}).copy()\n",
        "policy_used[\"Date\"] = policy_used.index\n",
        "\n",
        "def attach_sign(df):\n",
        "    if df is None or df.empty: return df\n",
        "    return df.reset_index(drop=True).merge(policy_used[[\"Date\",\"sign\"]].rename(columns={\"Date\":\"event_date\"}),\n",
        "                                           on=\"event_date\", how=\"left\")\n",
        "\n",
        "pure_by_sign  = attach_sign(pure_CAR_by_event)\n",
        "model_by_sign = attach_sign(model_CAR_by_event)\n",
        "\n",
        "def summarize_sign(df):\n",
        "    if df is None or df.empty: return None, None, None\n",
        "    pos = df[df[\"sign\"]==1]; neg = df[df[\"sign\"]==-1]\n",
        "    def statit(g):\n",
        "        out = g.groupby([\"bucket\",\"window\"])[\"CAR\"].agg([\"count\",\"mean\",\"std\"]).reset_index()\n",
        "        out[\"se\"] = out[\"std\"] / np.sqrt(out[\"count\"])\n",
        "        out[\"t_stat\"] = out[\"mean\"] / out[\"se\"].replace(0,np.nan)\n",
        "        order = pd.Categorical(out[\"window\"], [\"[-1,+1]\",\"[-3,+3]\",\"[0,+5]\"], ordered=True)\n",
        "        return out.assign(window=order).sort_values([\"bucket\",\"window\"])\n",
        "    pos_s, neg_s = statit(pos), statit(neg)\n",
        "    if pos_s is not None and neg_s is not None and not pos_s.empty and not neg_s.empty:\n",
        "        m = pos_s.merge(neg_s, on=[\"bucket\",\"window\"], suffixes=(\"_pos\",\"_neg\"))\n",
        "        m[\"diff_mean\"] = m[\"mean_pos\"] - m[\"mean_neg\"]\n",
        "        m[\"se_diff\"] = np.sqrt((m[\"std_pos\"]**2/m[\"count_pos\"]) + (m[\"std_neg\"]**2/m[\"count_neg\"]))\n",
        "        m[\"t_stat_diff\"] = m[\"diff_mean\"] / m[\"se_diff\"].replace(0,np.nan)\n",
        "        diff = m[[\"bucket\",\"window\",\"count_pos\",\"count_neg\",\"mean_pos\",\"mean_neg\",\"diff_mean\",\"t_stat_diff\"]]\n",
        "    else:\n",
        "        diff = None\n",
        "    return pos_s, neg_s, diff\n",
        "\n",
        "pure_pos, pure_neg, pure_diff   = summarize_sign(pure_by_sign)\n",
        "model_pos, model_neg, model_diff = summarize_sign(model_by_sign)\n",
        "\n",
        "# Save event workbook\n",
        "with pd.ExcelWriter(os.path.join(OUT,\"event_study_results.xlsx\"), engine=\"openpyxl\") as xw:\n",
        "    buckets.reset_index().rename(columns={\"index\":\"Date\"}).to_excel(xw, sheet_name=\"bucket_returns\", index=False)\n",
        "    pol.to_excel(xw, sheet_name=\"policy_used\")\n",
        "    pure_AR.to_excel(xw, sheet_name=\"pure_AR_profile\")\n",
        "    (pure_CAR_mean if not pure_CAR_mean.empty else pd.DataFrame()).to_excel(xw, sheet_name=\"pure_CAR\")\n",
        "    if not pure_CAR_by_event.empty: pure_CAR_by_event.to_excel(xw, sheet_name=\"pure_CAR_by_event\", index=False)\n",
        "    model_AR.to_excel(xw, sheet_name=\"model_AR_profile\")\n",
        "    (model_CAR_mean if not model_CAR_mean.empty else pd.DataFrame()).to_excel(xw, sheet_name=\"model_CAR\")\n",
        "    if not model_CAR_by_event.empty: model_CAR_by_event.to_excel(xw, sheet_name=\"model_CAR_by_event\", index=False)\n",
        "    if pure_pos is not None:  pure_pos.to_excel(xw, sheet_name=\"pure_pos\", index=False)\n",
        "    if pure_neg is not None:  pure_neg.to_excel(xw, sheet_name=\"pure_neg\", index=False)\n",
        "    if pure_diff is not None: pure_diff.to_excel(xw, sheet_name=\"pure_pos_minus_neg\", index=False)\n",
        "    if model_pos is not None:  model_pos.to_excel(xw, sheet_name=\"model_pos\", index=False)\n",
        "    if model_neg is not None:  model_neg.to_excel(xw, sheet_name=\"model_neg\", index=False)\n",
        "    if model_diff is not None: model_diff.to_excel(xw, sheet_name=\"model_pos_minus_neg\", index=False)\n",
        "    spec = pd.DataFrame({\n",
        "        \"parameter\":[\"estimation_window\",\"profile_band\",\"windows\",\"regressors\",\"car_convention\",\"notes\"],\n",
        "        \"value\":[\n",
        "            \"[-250,-31]\",\n",
        "            \"[-10,+10]\",\n",
        "            \", \".join([f\"{k}:{v}\" for k,v in WINDOWS.items()]),\n",
        "            \", \".join(regressors),\n",
        "            \"CAR = sum of ARs (textbook)\",\n",
        "            \"Buckets rebuilt from Workpaper_2 with updated mapping; USD price = local / (USD/local).\"\n",
        "        ]\n",
        "    })\n",
        "    spec.to_excel(xw, sheet_name=\"spec_info\", index=False)\n",
        "\n",
        "# Plots: AR profiles & CAR bars\n",
        "def line_plot(df, title, fname, ylabel=\"Mean AR\"):\n",
        "    if df is None or df.empty: return\n",
        "    plt.figure(figsize=(10,6))\n",
        "    for c in df.columns:\n",
        "        plt.plot(df.index, df[c], label=c)\n",
        "    plt.title(title); plt.xlabel(\"Event Day (k)\"); plt.ylabel(ylabel)\n",
        "    plt.legend(); plt.tight_layout()\n",
        "    plt.savefig(os.path.join(OUT, fname), dpi=150); plt.close()\n",
        "\n",
        "line_plot(pure_AR[bucket_cols],  \"Pure Event Study: Mean AR (âˆ’10,+10)\",  \"pure_AR_profile.png\")\n",
        "line_plot(model_AR[bucket_cols], \"Model-Adjusted: Mean AR (âˆ’10,+10)\",    \"model_AR_profile.png\")\n",
        "\n",
        "def car_barplot(car_mean_table, title, fname):\n",
        "    if car_mean_table is None or car_mean_table.empty: return\n",
        "    fig, axes = plt.subplots(1, len(WINDOWS), figsize=(13,4), sharey=True)\n",
        "    for ax, (wname, _) in zip(axes, WINDOWS.items()):\n",
        "        if wname not in car_mean_table.columns:\n",
        "            ax.set_title(wname); ax.set_axis_off(); continue\n",
        "        sub = car_mean_table[wname].reindex(bucket_cols)\n",
        "        ax.bar(range(len(sub.index)), sub.values)\n",
        "        ax.set_xticks(range(len(sub.index)))\n",
        "        ax.set_xticklabels(sub.index, rotation=20)\n",
        "        ax.set_title(wname); ax.axhline(0, linewidth=0.8, color='black')\n",
        "    fig.suptitle(title); fig.tight_layout(rect=[0,0,1,0.95])\n",
        "    fig.savefig(os.path.join(OUT, fname), dpi=150); plt.close(fig)\n",
        "\n",
        "car_barplot(pure_CAR_mean,  \"Pure Event Study: Mean CARs (sum of ARs)\",   \"pure_CAR_bars.png\")\n",
        "car_barplot(model_CAR_mean, \"Model-Adjusted: Mean CARs (sum of ARs)\",     \"model_CAR_bars.png\")\n",
        "\n",
        "def car_by_sign_bar(model_pos, model_neg, title, fname):\n",
        "    if model_pos is None or model_neg is None or model_pos.empty or model_neg.empty: return\n",
        "    piv_pos = model_pos.pivot(index=\"bucket\", columns=\"window\", values=\"mean\")\n",
        "    piv_neg = model_neg.pivot(index=\"bucket\", columns=\"window\", values=\"mean\")\n",
        "    fig, axes = plt.subplots(1, len(WINDOWS), figsize=(13,4), sharey=True)\n",
        "    for ax, w in zip(axes, [\"[-1,+1]\",\"[-3,+3]\",\"[0,+5]\"]):\n",
        "        pos_series = piv_pos[w] if w in piv_pos.columns else None\n",
        "        neg_series = piv_neg[w] if w in piv_pos.columns else None\n",
        "        if pos_series is None or neg_series is None:\n",
        "            ax.set_title(w); ax.set_axis_off(); continue\n",
        "        idxs = bucket_cols\n",
        "        pos_vals = [pos_series.get(b, np.nan) for b in idxs]\n",
        "        neg_vals = [neg_series.get(b, np.nan) for b in idxs]\n",
        "        x = np.arange(len(idxs)); bw = 0.35\n",
        "        ax.bar(x - bw/2, pos_vals, width=bw, label=\"+1 (Favorable)\")\n",
        "        ax.bar(x + bw/2, neg_vals, width=bw, label=\"âˆ’1 (Unfavorable)\")\n",
        "        ax.set_xticks(x); ax.set_xticklabels(idxs, rotation=20)\n",
        "        ax.axhline(0, linewidth=0.8, color='black'); ax.set_title(w)\n",
        "    handles, labels = axes[-1].get_legend_handles_labels()\n",
        "    fig.legend(handles, labels, loc=\"upper center\", ncol=2)\n",
        "    fig.suptitle(title); fig.tight_layout(rect=[0,0,1,0.92])\n",
        "    fig.savefig(os.path.join(OUT, fname), dpi=150); plt.close(fig)\n",
        "\n",
        "car_by_sign_bar(model_pos, model_neg, \"Model-Adjusted CARs by Policy Sign (sum of ARs)\", \"model_CAR_by_sign_bars.png\")\n",
        "\n",
        "print(\"âœ… Part 2 saved:\", os.path.join(OUT,\"event_study_results.xlsx\"))\n",
        "\n",
        "# ------------------ Risk deep-dive ------------------\n",
        "def drawdown_series(r):\n",
        "    r = r.fillna(0.0)\n",
        "    wealth = (1 + r).cumprod()\n",
        "    peak = wealth.cummax()\n",
        "    dd = wealth/peak - 1.0\n",
        "    # drawdown duration (days since last peak)\n",
        "    is_new_peak = wealth == peak\n",
        "    dur = np.zeros(len(wealth), dtype=int)\n",
        "    c = 0\n",
        "    for i, flag in enumerate(is_new_peak):\n",
        "        c = 0 if flag else c + 1\n",
        "        dur[i] = c\n",
        "    return wealth, dd, pd.Series(dur, index=r.index)\n",
        "\n",
        "def ulcer_index(dd):\n",
        "    if dd.empty: return np.nan\n",
        "    x = (dd * 100.0).values\n",
        "    return float(np.sqrt(np.mean(x**2)))\n",
        "\n",
        "def var_es(x, alpha=0.95):\n",
        "    s = pd.Series(x).dropna()\n",
        "    if s.empty: return np.nan, np.nan\n",
        "    q = s.quantile(1 - alpha)  # left-tail (usually negative)\n",
        "    es = s[s <= q].mean() if (s <= q).any() else np.nan\n",
        "    return float(q), float(es)\n",
        "\n",
        "rows, dist_rows, tail_rows = [], [], []\n",
        "dd_details, dur_details = {}, {}\n",
        "\n",
        "for b in bucket_cols:\n",
        "    r = buckets[b].astype(float)\n",
        "\n",
        "    # distribution moments\n",
        "    dist_rows.append({\"bucket\": b, \"skew\": r.dropna().skew(), \"excess_kurtosis\": r.dropna().kurt()})\n",
        "\n",
        "    # tail risk\n",
        "    VaR95, ES95 = var_es(r, 0.95)\n",
        "    VaR99, ES99 = var_es(r, 0.99)\n",
        "    tail_rows.append({\"bucket\": b, \"VaR95\": VaR95, \"ES95\": ES95, \"VaR99\": VaR99, \"ES99\": ES99})\n",
        "\n",
        "    # drawdown stats\n",
        "    wealth, dd, dur = drawdown_series(r)\n",
        "    dd_details[b], dur_details[b] = dd, dur\n",
        "    mdd = dd.min() if not dd.empty else np.nan\n",
        "    ui = ulcer_index(dd)\n",
        "    longest = int(dur.max()) if not dur.empty else np.nan\n",
        "    avgdur = float(dur[dur>0].mean()) if (dur>0).any() else 0.0\n",
        "    rows.append({\"bucket\": b, \"max_drawdown\": float(mdd), \"ulcer_index\": ui,\n",
        "                 \"longest_drawdown_days\": longest, \"avg_drawdown_days\": avgdur})\n",
        "\n",
        "risk_drawdowns = pd.concat(dd_details, axis=1) if dd_details else pd.DataFrame()\n",
        "risk_durations = pd.concat(dur_details, axis=1) if dur_details else pd.DataFrame()\n",
        "risk_dd_summary = pd.DataFrame(rows).set_index(\"bucket\")\n",
        "risk_dist = pd.DataFrame(dist_rows).set_index(\"bucket\")\n",
        "risk_tail = pd.DataFrame(tail_rows).set_index(\"bucket\")\n",
        "\n",
        "with pd.ExcelWriter(os.path.join(OUT,\"risk_deep_dive.xlsx\"), engine=\"openpyxl\") as xw:\n",
        "    buckets.to_excel(xw, sheet_name=\"bucket_returns\")\n",
        "    risk_dd_summary.to_excel(xw, sheet_name=\"drawdown_metrics\")\n",
        "    risk_dist.to_excel(xw, sheet_name=\"distribution_moments\")\n",
        "    risk_tail.to_excel(xw, sheet_name=\"tail_metrics\")\n",
        "    if not risk_drawdowns.empty: risk_drawdowns.to_excel(xw, sheet_name=\"drawdown_series\")\n",
        "    if not risk_durations.empty: risk_durations.to_excel(xw, sheet_name=\"drawdown_duration_series\")\n",
        "\n",
        "# Plots for risk deep-dive\n",
        "# 1) Histograms with normal overlay\n",
        "for b in bucket_cols:\n",
        "    r = buckets[b].dropna().astype(float)\n",
        "    if r.empty: continue\n",
        "    mu, sd = r.mean(), r.std()\n",
        "    xs = np.linspace(r.min()*1.2, r.max()*1.2, 300)\n",
        "    pdf = (1/(sd*np.sqrt(2*np.pi))) * np.exp(-0.5*((xs-mu)/sd)**2) if sd>0 else np.zeros_like(xs)\n",
        "    plt.figure(figsize=(8,5))\n",
        "    plt.hist(r.values, bins=60, density=True, alpha=0.6, label=\"Empirical\")\n",
        "    if sd>0: plt.plot(xs, pdf, linewidth=1.5, label=\"Normal(Î¼,Ïƒ)\")\n",
        "    plt.title(f\"Daily Return Distribution â€” {b}\\n(skew={r.skew():.2f}, ex.kurt={r.kurt():.2f})\")\n",
        "    plt.xlabel(\"Daily Return\"); plt.ylabel(\"Density\"); plt.legend(); plt.tight_layout()\n",
        "    plt.savefig(os.path.join(OUT, f\"hist_{b}.png\"), dpi=150); plt.close()\n",
        "\n",
        "# 2) Drawdown series\n",
        "for b in bucket_cols:\n",
        "    dd = risk_drawdowns[b].dropna() if b in risk_drawdowns else pd.Series(dtype=float)\n",
        "    if dd.empty: continue\n",
        "    plt.figure(figsize=(10,4))\n",
        "    plt.plot(dd.index, dd.values)\n",
        "    plt.title(f\"Drawdown â€” {b}\"); plt.xlabel(\"Date\"); plt.ylabel(\"Drawdown\")\n",
        "    plt.tight_layout(); plt.savefig(os.path.join(OUT, f\"risk_dd_{b}.png\"), dpi=150); plt.close()\n",
        "\n",
        "# 3) Drawdown duration bars\n",
        "plt.figure(figsize=(9,5))\n",
        "x = np.arange(len(bucket_cols))\n",
        "max_d = [risk_dd_summary.loc[b, \"longest_drawdown_days\"] for b in bucket_cols]\n",
        "avg_d = [risk_dd_summary.loc[b, \"avg_drawdown_days\"] for b in bucket_cols]\n",
        "bw = 0.35\n",
        "plt.bar(x - bw/2, max_d, width=bw, label=\"Max duration\")\n",
        "plt.bar(x + bw/2, avg_d, width=bw, label=\"Avg duration\")\n",
        "plt.xticks(x, bucket_cols, rotation=15)\n",
        "plt.title(\"Drawdown Duration â€” Max vs Average (days)\")\n",
        "plt.legend(); plt.tight_layout()\n",
        "plt.savefig(os.path.join(OUT, \"drawdown_duration_bars.png\"), dpi=150); plt.close()\n",
        "\n",
        "# 4) Tail risk bars\n",
        "def bar_two(df, cols, title, fname):\n",
        "    fig, ax = plt.subplots(figsize=(9,5))\n",
        "    x = np.arange(len(df.index)); bw = 0.35\n",
        "    ax.bar(x - bw/2, df[cols[0]].values, width=bw, label=cols[0])\n",
        "    ax.bar(x + bw/2, df[cols[1]].values, width=bw, label=cols[1])\n",
        "    ax.set_xticks(x); ax.set_xticklabels(df.index, rotation=15)\n",
        "    ax.axhline(0, color='black', linewidth=0.8)\n",
        "    ax.set_title(title); ax.legend(); plt.tight_layout()\n",
        "    fig.savefig(os.path.join(OUT, fname), dpi=150); plt.close(fig)\n",
        "\n",
        "bar_two(risk_tail, [\"VaR95\",\"ES95\"], \"Tail Risk (95%) â€” VaR & ES (daily)\", \"tail_95_bars.png\")\n",
        "bar_two(risk_tail, [\"VaR99\",\"ES99\"], \"Tail Risk (99%) â€” VaR & ES (daily)\", \"tail_99_bars.png\")\n",
        "\n",
        "print(\"\\nðŸŽ¯ Done. All outputs in:\", OUT)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IEywAvgDXdDu",
        "outputId": "c39bb654-1198-4ab3-d128-bc438ac52fd5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==> Output folder: /content/drive/MyDrive/sep15.3.0\n",
            "âœ“ Found Workpaper_2 (prices + FX): /content/drive/MyDrive/2ndtrial/Workpaper_2.xlsx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-699379097.py:31: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
            "  dt = pd.to_datetime(s, errors='coerce', dayfirst=prefer_dayfirst, infer_datetime_format=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns after cleaning: ['Vestas Wind', 'Oersted AS', 'USD/DKK', 'Nel ASA', 'USD/NOK', 'Bloom Energy', 'NextEra Energy', 'Plug Power', 'AC Energy Philippines Inc', 'USD/PHP', 'Tata Power', 'USD/INR', 'China Longyuan Power Group', 'USD/HKD', 'ACWA Power', 'USD/SAR', 'Adani Green Energy', 'Sembcorp', 'USD/SGD']\n",
            "âœ… Saved per-stock USD prices/returns for audit.\n",
            "âœ… Saved bucket returns: /content/drive/MyDrive/sep15.3.0/bucket_log_returns.xlsx\n",
            "âœ“ Found FF5.csv: /content/drive/MyDrive/FF5.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-699379097.py:31: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
            "  dt = pd.to_datetime(s, errors='coerce', dayfirst=prefer_dayfirst, infer_datetime_format=True)\n",
            "/tmp/ipython-input-699379097.py:34: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
            "  dt2 = pd.to_datetime(pd.Series(s)[bad], errors='coerce',\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ Found brent_daily.xlsx: /content/drive/MyDrive/brent_daily.xlsx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-699379097.py:31: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
            "  dt = pd.to_datetime(s, errors='coerce', dayfirst=prefer_dayfirst, infer_datetime_format=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ Found DGS10: /content/drive/MyDrive/2ndtrial/DGS10.xlsx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-699379097.py:31: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
            "  dt = pd.to_datetime(s, errors='coerce', dayfirst=prefer_dayfirst, infer_datetime_format=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ Found ACWI: /content/drive/MyDrive/2ndtrial/ACWI.xlsx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-699379097.py:31: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
            "  dt = pd.to_datetime(s, errors='coerce', dayfirst=prefer_dayfirst, infer_datetime_format=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ Found policy_global_expanded: /content/drive/MyDrive/2ndtrial/policy_global_expanded.xlsx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-699379097.py:31: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
            "  dt = pd.to_datetime(s, errors='coerce', dayfirst=prefer_dayfirst, infer_datetime_format=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Part 1 saved: /content/drive/MyDrive/sep15.3.0/rolling_metrics_252d.xlsx\n",
            "Events available: 96\n",
            "âœ… Part 2 saved: /content/drive/MyDrive/sep15.3.0/event_study_results.xlsx\n",
            "\n",
            "ðŸŽ¯ Done. All outputs in: /content/drive/MyDrive/sep15.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6BIRzH0-ErJa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Robustness Checks (Spec Comparison + Placebo)\n",
        "\n",
        "\n",
        "import os, numpy as np, pandas as pd, statsmodels.api as sm\n",
        "\n",
        "# -----------------------------\n",
        "# Paths / files\n",
        "# -----------------------------\n",
        "OUT_DIR = \"/content/drive/MyDrive/2ndtrial/outputs_portfolio\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "ES_WB   = os.path.join(OUT_DIR, \"event_study_results.xlsx\")  # contains 'bucket_returns' + 'policy_used'\n",
        "ROOT    = \"/content/drive/MyDrive\"                           # FF5/Brent/DGS10/ACWI live here (per your setup)\n",
        "\n",
        "FF5_PATH   = os.path.join(ROOT, \"FF5.csv\")\n",
        "BRENT_PATH = os.path.join(ROOT, \"brent_daily.xlsx\")\n",
        "DGS10_PATH = os.path.join(ROOT, \"dgs10_daily.xlsx\")\n",
        "ACWI_PATH  = os.path.join(ROOT, \"ACWI_DAILY.xlsx\") if os.path.exists(os.path.join(ROOT, \"ACWI_DAILY.xlsx\")) else os.path.join(ROOT, \"ACWI.xlsx\")\n",
        "\n",
        "ROB_XLSX   = os.path.join(OUT_DIR, \"robustness_checks.xlsx\")\n",
        "\n",
        "# -----------------------------\n",
        "# Helpers\n",
        "# -----------------------------\n",
        "def parse_date(s):\n",
        "    return pd.to_datetime(s, errors=\"coerce\").dt.normalize()\n",
        "\n",
        "def tstat_of_mean(vals):\n",
        "    vals = np.asarray(vals, dtype=float)\n",
        "    vals = vals[~np.isnan(vals)]\n",
        "    n = len(vals)\n",
        "    if n == 0:\n",
        "        return np.nan, np.nan, np.nan, np.nan\n",
        "    mean = vals.mean()\n",
        "    std = vals.std(ddof=1) if n > 1 else 0.0\n",
        "    se  = std / np.sqrt(n) if n > 1 else np.nan\n",
        "    t   = mean / se if se and se > 0 else np.nan\n",
        "    return mean, std, se, t\n",
        "\n",
        "def summarize_by_bucket(car_df):\n",
        "    # car_df: columns [bucket, window, event_id?, CAR]\n",
        "    out = []\n",
        "    for (b,w), g in car_df.groupby([\"bucket\",\"window\"]):\n",
        "        mean,std,se,t = tstat_of_mean(g[\"CAR\"])\n",
        "        out.append({\"bucket\":b,\"window\":w,\"mean\":mean,\"std\":std,\"se\":se,\"t_stat\":t, \"N\":len(g)})\n",
        "    return pd.DataFrame(out)\n",
        "\n",
        "def build_windows():\n",
        "    # fixed labels as used across your work\n",
        "    return {\"[-1,+1]\":(-1,+1), \"[-3,+3]\":(-3,+3), \"[0,+5]\":(0,+5)}\n",
        "\n",
        "def pick_col(df, candidates):\n",
        "    for c in candidates:\n",
        "        if c in df.columns: return c\n",
        "    return None\n",
        "\n",
        "# -----------------------------\n",
        "# Load from event workbook\n",
        "# -----------------------------\n",
        "xl = pd.ExcelFile(ES_WB)\n",
        "brets  = xl.parse(\"bucket_returns\")\n",
        "policy = xl.parse(\"policy_used\")\n",
        "\n",
        "# bucket returns\n",
        "dcol = [c for c in brets.columns if c.lower()==\"date\"]\n",
        "if not dcol: raise RuntimeError(\"bucket_returns sheet must include 'Date'.\")\n",
        "dcol = dcol[0]\n",
        "brets[dcol] = parse_date(brets[dcol])\n",
        "brets = brets.set_index(dcol).sort_index()\n",
        "buckets = [c for c in brets.columns if c.lower() not in (\"date\",\"rf\",\"mktrf\")]\n",
        "\n",
        "# policy events\n",
        "pdate_col = [c for c in policy.columns if c.lower() in (\"date\",\"event_date\")]\n",
        "psign_col = [c for c in policy.columns if c.lower() in (\"globalpolicy_0\",\"policy_sign\",\"sign\")]\n",
        "if not pdate_col or not psign_col:\n",
        "    raise RuntimeError(\"policy_used sheet must include Date/Event_Date and GlobalPolicy_0/Policy_Sign.\")\n",
        "events = policy[[pdate_col[0], psign_col[0]]].copy()\n",
        "events.columns = [\"event_date\",\"policy_sign\"]\n",
        "events[\"event_date\"] = parse_date(events[\"event_date\"])\n",
        "events = events[events[\"policy_sign\"].isin([-1,1])].dropna().drop_duplicates(\"event_date\").sort_values(\"event_date\")\n",
        "if events.empty: raise RuntimeError(\"No Â±1 policy events found in policy_used.\")\n",
        "\n",
        "# -----------------------------\n",
        "# Load regressors (FF5, Brent, DGS10, ACWI) â€” robust to header variants\n",
        "# -----------------------------\n",
        "# FF5\n",
        "ff = pd.read_csv(FF5_PATH)\n",
        "ff[\"Date\"] = parse_date(ff[\"Date\"])\n",
        "ff = ff.set_index(\"Date\").sort_index()\n",
        "ff = ff.rename(columns={\"Mkt-RF\":\"Mkt_RF\"})  # standardize\n",
        "# ensure decimals, not %\n",
        "for c in [\"Mkt_RF\",\"SMB\",\"HML\",\"RMW\",\"CMA\",\"RF\"]:\n",
        "    if c in ff.columns and ff[c].abs().median() > 1:\n",
        "        ff[c] = ff[c] / 100.0\n",
        "\n",
        "# Brent\n",
        "br = pd.read_excel(BRENT_PATH)\n",
        "date_col_br = pick_col(br, [\"observation_date\",\"Date\",\"date\"])\n",
        "if date_col_br is None: raise KeyError(\"brent_daily.xlsx must have a date column.\")\n",
        "br[date_col_br] = parse_date(br[date_col_br])\n",
        "br = br.set_index(date_col_br).sort_index()\n",
        "br_ret_col = pick_col(br, [\"Brent_ret\",\"Brent_re\",\"Log_returns\",\"log_returns\",\"ret\",\"return\"])\n",
        "if br_ret_col is not None:\n",
        "    br[\"Brent_ret\"] = pd.to_numeric(br[br_ret_col], errors=\"coerce\")\n",
        "else:\n",
        "    price_col = pick_col(br, [\"DCOILBRENTEU\",\"Price\",\"Close\",\"Brent\"])\n",
        "    if price_col is None: raise KeyError(\"Brent: need return column or price to compute.\")\n",
        "    br[\"Brent_ret\"] = pd.to_numeric(br[price_col], errors=\"coerce\").pct_change()\n",
        "br = br[[\"Brent_ret\"]]\n",
        "\n",
        "# DGS10\n",
        "dgs = pd.read_excel(DGS10_PATH)\n",
        "date_col_dgs = pick_col(dgs, [\"observation_date\",\"Date\",\"date\"])\n",
        "if date_col_dgs is None: raise KeyError(\"dgs10_daily.xlsx must have a date column.\")\n",
        "dgs[date_col_dgs] = parse_date(dgs[date_col_dgs])\n",
        "dgs = dgs.set_index(date_col_dgs).sort_index()\n",
        "dgs_delta_col = pick_col(dgs, [\"DGS10_deltabp\",\"Delta_bp\",\"delta_bp\",\"delta\"])\n",
        "if dgs_delta_col is not None:\n",
        "    dgs[\"DGS10_deltabp\"] = pd.to_numeric(dgs[dgs_delta_col], errors=\"coerce\")\n",
        "else:\n",
        "    lvl = pick_col(dgs, [\"DGS10\",\"Yield\",\"Close\"])\n",
        "    if lvl is None: raise KeyError(\"DGS10: need Delta_bp or level to compute.\")\n",
        "    dgs[\"DGS10_deltabp\"] = pd.to_numeric(dgs[lvl], errors=\"coerce\").diff() * 100.0\n",
        "dgs = dgs[[\"DGS10_deltabp\"]]\n",
        "\n",
        "# ACWI\n",
        "ac = pd.read_excel(ACWI_PATH)\n",
        "date_col_ac = pick_col(ac, [\"Date\",\"date\",\"observation_date\"])\n",
        "if date_col_ac is None: raise KeyError(\"ACWI file must have a date column.\")\n",
        "ac[date_col_ac] = parse_date(ac[date_col_ac])\n",
        "ac = ac.set_index(date_col_ac).sort_index()\n",
        "ac_ret_col = pick_col(ac, [\"ACWI_ret\",\"acwi_ret\"])\n",
        "if ac_ret_col is not None:\n",
        "    ac[\"ACWI_ret\"] = pd.to_numeric(ac[ac_ret_col], errors=\"coerce\")\n",
        "else:\n",
        "    price_col_ac = pick_col(ac, [\"Price\",\"Close\",\"Adj Close\",\"ACWI\"])\n",
        "    if price_col_ac is None: raise KeyError(\"ACWI: need ACWI_ret or price to compute.\")\n",
        "    ac[\"ACWI_ret\"] = pd.to_numeric(ac[price_col_ac], errors=\"coerce\").pct_change()\n",
        "ac = ac[[\"ACWI_ret\"]]\n",
        "\n",
        "# Master regressors\n",
        "X_all = (\n",
        "    ff[[\"Mkt_RF\",\"SMB\",\"HML\",\"RMW\",\"CMA\",\"RF\"]]\n",
        "    .merge(br,  left_index=True, right_index=True, how=\"outer\")\n",
        "    .merge(dgs, left_index=True, right_index=True, how=\"outer\")\n",
        "    .merge(ac,  left_index=True, right_index=True, how=\"outer\")\n",
        ").sort_index()\n",
        "\n",
        "# -----------------------------\n",
        "# Build abnormal returns (residuals): two specs\n",
        "#   Spec A (Original): FF5 + Brent + DGS10 + ACWI\n",
        "#   Spec B (Macro-only): Brent + DGS10 + ACWI\n",
        "# -----------------------------\n",
        "master = brets.merge(X_all, left_index=True, right_index=True, how=\"left\").dropna()\n",
        "reg_A = [\"Mkt_RF\",\"SMB\",\"HML\",\"RMW\",\"CMA\",\"Brent_ret\",\"DGS10_deltabp\",\"ACWI_ret\"]\n",
        "reg_B = [\"Brent_ret\",\"DGS10_deltabp\",\"ACWI_ret\"]\n",
        "\n",
        "# make excess returns per bucket\n",
        "for b in buckets:\n",
        "    master[f\"{b}_excess\"] = master[b] - master[\"RF\"]\n",
        "\n",
        "def residuals_for(spec_cols):\n",
        "    AR = pd.DataFrame(index=master.index)\n",
        "    for b in buckets:\n",
        "        y  = master[f\"{b}_excess\"]\n",
        "        Xr = sm.add_constant(master[spec_cols])\n",
        "        res = sm.OLS(y, Xr, missing=\"drop\").fit()\n",
        "        AR[b] = res.resid\n",
        "    return AR\n",
        "\n",
        "AR_A = residuals_for(reg_A)  # Original model ARs\n",
        "AR_B = residuals_for(reg_B)  # Macro-only ARs\n",
        "\n",
        "# -----------------------------\n",
        "# Event windows and CARs\n",
        "# -----------------------------\n",
        "win_bounds = build_windows()\n",
        "\n",
        "def car_table(AR_df):\n",
        "    # AR_df indexed by date; columns = buckets\n",
        "    AR_long = AR_df.reset_index().rename(columns={AR_df.index.name or \"index\":\"Date\"})\n",
        "    AR_long[\"Date\"] = parse_date(AR_long[\"Date\"])\n",
        "    arL = AR_long.melt(id_vars=[\"Date\"], value_vars=buckets, var_name=\"bucket\", value_name=\"AR\")\n",
        "    arL = arL.set_index(\"Date\").sort_index()\n",
        "    all_dates = arL.index.unique()\n",
        "\n",
        "    rows = []\n",
        "    for e in events[\"event_date\"]:\n",
        "        if e not in all_dates:\n",
        "            continue\n",
        "        e_loc = all_dates.get_loc(e)\n",
        "        for wlab, (k0,k1) in win_bounds.items():\n",
        "            idx = [e_loc+k for k in range(k0,k1+1) if 0 <= e_loc+k < len(all_dates)]\n",
        "            dates_win = all_dates[idx]\n",
        "            g = (arL.loc[dates_win].reset_index()\n",
        "                    .groupby(\"bucket\", as_index=False)[\"AR\"].sum())\n",
        "            g[\"window\"] = wlab\n",
        "            g[\"event_date\"] = e\n",
        "            g.rename(columns={\"AR\":\"CAR\"}, inplace=True)\n",
        "            rows.append(g)\n",
        "    if not rows:\n",
        "        return pd.DataFrame(columns=[\"bucket\",\"window\",\"event_date\",\"CAR\"])\n",
        "    out = pd.concat(rows, ignore_index=True)\n",
        "    return out\n",
        "\n",
        "CAR_A = car_table(AR_A)\n",
        "CAR_B = car_table(AR_B)\n",
        "\n",
        "SUM_A = summarize_by_bucket(CAR_A)\n",
        "SUM_B = summarize_by_bucket(CAR_B)\n",
        "\n",
        "# Comparison table\n",
        "cmp = (SUM_A.rename(columns={\"mean\":\"mean_FF5\",\"std\":\"std_FF5\",\"se\":\"se_FF5\",\"t_stat\":\"t_FF5\",\"N\":\"N_FF5\"})\n",
        "            .merge(SUM_B.rename(columns={\"mean\":\"mean_MACRO\",\"std\":\"std_MACRO\",\"se\":\"se_MACRO\",\"t_stat\":\"t_MACRO\",\"N\":\"N_MACRO\"}),\n",
        "                   on=[\"bucket\",\"window\"], how=\"inner\"))\n",
        "cmp[\"diff\"]      = cmp[\"mean_MACRO\"] - cmp[\"mean_FF5\"]\n",
        "cmp[\"abs_diff\"]  = cmp[\"diff\"].abs()\n",
        "cmp[\"pct_change\"] = np.where(cmp[\"mean_FF5\"]!=0, cmp[\"diff\"]/cmp[\"mean_FF5\"], np.nan)\n",
        "\n",
        "# -----------------------------\n",
        "# Placebo test (200 draws, two-sided p)\n",
        "# -----------------------------\n",
        "rng = np.random.default_rng(42)\n",
        "\n",
        "# candidate non-event trading days (use AR_A domain)\n",
        "AR_A_long = AR_A.reset_index().rename(columns={AR_A.index.name or \"index\":\"Date\"})\n",
        "AR_A_long[\"Date\"] = parse_date(AR_A_long[\"Date\"])\n",
        "all_days = AR_A_long[\"Date\"].drop_duplicates().sort_values()\n",
        "event_days = set(events[\"event_date\"])\n",
        "candidates = [d for d in all_days if d not in event_days]\n",
        "n_events = len(events)\n",
        "draws = 200\n",
        "\n",
        "def placebo_once(AR_df):\n",
        "    AR_long = AR_df.reset_index().rename(columns={AR_df.index.name or \"index\":\"Date\"})\n",
        "    AR_long[\"Date\"] = parse_date(AR_long[\"Date\"])\n",
        "    arL = AR_long.melt(id_vars=[\"Date\"], value_vars=buckets, var_name=\"bucket\", value_name=\"AR\")\n",
        "    arL = arL.set_index(\"Date\").sort_index()\n",
        "    all_dates = arL.index.unique()\n",
        "\n",
        "    fake = rng.choice(candidates, size=n_events, replace=False)\n",
        "    rows = []\n",
        "    for e in fake:\n",
        "        if e not in all_dates:\n",
        "            continue\n",
        "        e_loc = all_dates.get_loc(e)\n",
        "        for wlab,(k0,k1) in win_bounds.items():\n",
        "            idx = [e_loc+k for k in range(k0,k1+1) if 0 <= e_loc+k < len(all_dates)]\n",
        "            dates_win = all_dates[idx]\n",
        "            g = (arL.loc[dates_win].reset_index()\n",
        "                    .groupby(\"bucket\", as_index=False)[\"AR\"].sum())\n",
        "            g[\"window\"] = wlab\n",
        "            g[\"event_date\"] = e\n",
        "            g.rename(columns={\"AR\":\"CAR\"}, inplace=True)\n",
        "            rows.append(g)\n",
        "    if not rows:\n",
        "        return pd.DataFrame()\n",
        "    return pd.concat(rows, ignore_index=True)\n",
        "\n",
        "def placebo_summary(AR_df):\n",
        "    dists = []\n",
        "    for _ in range(draws):\n",
        "        pc = placebo_once(AR_df)\n",
        "        if pc.empty: continue\n",
        "        s = summarize_by_bucket(pc)\n",
        "        s[\"draw\"] = _\n",
        "        dists.append(s[[\"draw\",\"bucket\",\"window\",\"mean\"]])\n",
        "    if not dists:\n",
        "        return pd.DataFrame(), pd.DataFrame()\n",
        "    dist = pd.concat(dists, ignore_index=True).rename(columns={\"mean\":\"mean_draw\"})\n",
        "    # summary tab (mean across draws) + 95% CI\n",
        "    tab = (dist.groupby([\"bucket\",\"window\"], as_index=False)\n",
        "               .agg(mean_placebo=(\"mean_draw\",\"mean\"),\n",
        "                    std_placebo=(\"mean_draw\",\"std\")))\n",
        "    ci  = (dist.groupby([\"bucket\",\"window\"])[\"mean_draw\"]\n",
        "               .quantile([0.025,0.975])\n",
        "               .unstack().reset_index().rename(columns={0.025:\"p2_5\",0.975:\"p97_5\"}))\n",
        "    tab = tab.merge(ci, on=[\"bucket\",\"window\"], how=\"left\")\n",
        "    return tab, dist\n",
        "\n",
        "pl_tab, pl_dist = placebo_summary(AR_A)\n",
        "\n",
        "if not pl_tab.empty:\n",
        "    tmp = cmp.merge(pl_tab, on=[\"bucket\",\"window\"], how=\"left\")\n",
        "    pvals = []\n",
        "    for _, row in tmp.iterrows():\n",
        "        b,w = row[\"bucket\"], row[\"window\"]\n",
        "        these = pl_dist[(pl_dist[\"bucket\"]==b) & (pl_dist[\"window\"]==w)][\"mean_draw\"].values\n",
        "        if len(these)==0 or pd.isna(row[\"mean_FF5\"]):\n",
        "            pvals.append(np.nan); continue\n",
        "        p = np.mean(np.abs(these) >= abs(row[\"mean_FF5\"]))   # two-sided\n",
        "        pvals.append(p)\n",
        "    tmp[\"placebo_p_two_sided\"] = pvals\n",
        "    placebo_out = tmp[[\"bucket\",\"window\",\"mean_FF5\",\"t_FF5\",\"mean_placebo\",\"p2_5\",\"p97_5\",\"placebo_p_two_sided\"]]\n",
        "else:\n",
        "    placebo_out = pd.DataFrame()\n",
        "\n",
        "# -----------------------------\n",
        "# Write Excel with conditional formatting\n",
        "# -----------------------------\n",
        "with pd.ExcelWriter(ROB_XLSX, engine=\"xlsxwriter\") as w:\n",
        "    # Spec comparison\n",
        "    cmp.sort_values([\"bucket\",\"window\"]).to_excel(w, sheet_name=\"spec_compare\", index=False)\n",
        "    ws = w.sheets[\"spec_compare\"]\n",
        "    last_row = len(cmp) + 1\n",
        "    # 3-color scale on abs_diff\n",
        "    ws.conditional_format(1, cmp.columns.get_loc(\"abs_diff\"), last_row, cmp.columns.get_loc(\"abs_diff\"),\n",
        "                          {\"type\":\"3_color_scale\"})\n",
        "    # data bar on pct_change\n",
        "    ws.conditional_format(1, cmp.columns.get_loc(\"pct_change\"), last_row, cmp.columns.get_loc(\"pct_change\"),\n",
        "                          {\"type\":\"data_bar\"})\n",
        "    # flag large absolute differences (>= 1% CAR)\n",
        "    ws.conditional_format(1, cmp.columns.get_loc(\"abs_diff\"), last_row, cmp.columns.get_loc(\"abs_diff\"),\n",
        "                          {\"type\":\"cell\",\"criteria\":\">=\",\"value\":0.01,\"format\":w.book.add_format({\"bg_color\":\"#FFC7CE\"})})\n",
        "\n",
        "    # Individual summaries\n",
        "    SUM_A.sort_values([\"bucket\",\"window\"]).to_excel(w, sheet_name=\"summary_FF5\", index=False)\n",
        "    SUM_B.sort_values([\"bucket\",\"window\"]).to_excel(w, sheet_name=\"summary_macro_only\", index=False)\n",
        "\n",
        "    # Placebo results\n",
        "    if not placebo_out.empty:\n",
        "        placebo_out.sort_values([\"bucket\",\"window\"]).to_excel(w, sheet_name=\"placebo_vs_real\", index=False)\n",
        "        ws2 = w.sheets[\"placebo_vs_real\"]\n",
        "        col_p = placebo_out.columns.get_loc(\"placebo_p_two_sided\")\n",
        "        ws2.conditional_format(1, col_p, len(placebo_out), col_p,\n",
        "                               {\"type\":\"cell\",\"criteria\":\"<=\",\"value\":0.05,\"format\":w.book.add_format({\"bg_color\":\"#C6EFCE\"})})\n",
        "\n",
        "print(f\"âœ… Robustness Excel saved: {ROB_XLSX}\")\n",
        "print(\"Sheets:\")\n",
        "print(\" - spec_compare: Original(FF5+macro) vs Macro-only comparison of CAR means (diffs, % change, t-stats, N)\")\n",
        "print(\" - summary_FF5 / summary_macro_only: per-spec CAR stats (mean, std, se, t)\")\n",
        "print(\" - placebo_vs_real: real CAR vs placebo distribution (95% range + two-sided p-value)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pNxfi4y44kQs",
        "outputId": "9dd99c94-cac5-48f4-b6dc-42dd375e8d0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Robustness Excel saved: /content/drive/MyDrive/2ndtrial/outputs_portfolio/robustness_checks.xlsx\n",
            "Sheets:\n",
            " - spec_compare: Original(FF5+macro) vs Macro-only comparison of CAR means (diffs, % change, t-stats, N)\n",
            " - summary_FF5 / summary_macro_only: per-spec CAR stats (mean, std, se, t)\n",
            " - placebo_vs_real: real CAR vs placebo distribution (95% range + two-sided p-value)\n"
          ]
        }
      ]
    }
  ]
}